---
title: 'Automating My GitHub Avatar Sync'
publishedAt: '2025-08-06'
summary: 'Turn GitHub into the source of truth for your website avatar. A Node + Vercel pipeline fetches, optimises (Sharp), hashes, and deploys your image with smart cache-busting.'
---

The slow part wasn’t updating my GitHub avatar — that’s quick thanks to GitHub’s built-in cropping. The real time-sink was **keeping my website avatar in sync**: converting to WebP, resizing for the circle mask, and pushing updates to the repo. I automated the whole thing by making **GitHub the source of truth** and letting my build pipeline do the rest.

## Architecture at a Glance

```mermaid
flowchart TD
  A[Push to main] --> B[Vercel Build]
  B --> C["Node script: fetch GitHub avatar (HTTPS)"]
  C --> D["Node script: optimise with Sharp (resize to WebP)"]
  D --> E[Compute content hash]
  E --> F["Keep filename - leverage Cloudflare/Vercel/browser cache"]
  E --> G["Write new hashed filename - instant cache-bust"]
  F --> H[Publish]
  G --> H[Publish]
  H --> I[Site serves latest avatar]
```

## How It Works
- **Source of truth**: I update my avatar through GitHub's UI (which already has a great circular crop).
- **Trigger**: A push to main kicks off a Vercel build.
- **Fetch**: A small Node.js script downloads the current GitHub avatar over HTTPS.
- **Optimise**: Another Node.js script uses Sharp to resize, convert to WebP, and strip excess metadata.
- **Cache-smart deploy**: The build computes a content hash. If the image bytes didn't change, the filename stays the same (cache hits). If it did change, the filename changes immediately (cache busts) so Cloudflare, Vercel, and browsers serve the new file right away.
- **Publish**: The optimised asset is written to the site's assets folder and deployed.

Credit: the content-hashing approach came from Claude Sonnet 3.7 and has been rock-solid.

## Why Node, Not Bash?

This pipeline started life in shell scripts but quickly moved to Node for reliability and portability:
- **HTTP fetching with retries and proper error handling** is simpler in Node.
- **Sharp gives consistent, high-quality image processing** without external system deps.
- **Cross-platform behaviour** (local dev, CI, Vercel) is predictable.

## Lessons Learned
- **Designate a source of truth**. Don't crop/convert in two places — let GitHub's avatar UI handle that job.
- **Hash the bytes, not the idea**. Human "this looks the same" isn't good enough; content hashing makes cache behaviour deterministic.
- **Automate the boring parts**. Image chores (resize/convert) are perfect for scripts; use your human time on design and writing.

## Related Reading
- [AI Coding Tools: My Journey From Frustration to Flow](/blog/ai-coding-tools) — how I decide what work to hand to automation.
- [Seasonal Avatar Borders](/blog/seasonal-avatar-borders) — another small automation that keeps the site feeling alive.

## Closing Thoughts

The best developer ergonomics often come from removing the micro-chores you’ve normalised. By treating GitHub as the master copy and letting Vercel + Node handle the rest, my site stays fresh without me opening an image editor.
